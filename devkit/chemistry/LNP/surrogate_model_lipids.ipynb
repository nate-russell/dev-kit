{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/bowang-lab/AGILE/raw/81900a160f60f8d0728896d347fe569c952d6bd9/data.zip -> 'C:\\\\Users\\\\ntrus\\\\AppData\\\\Local\\\\Temp\\\\tmp7ffjdr5z.zip'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500e6946923e4ae9b773fc750d04d4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/52.1M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'candidate_set_smiles_plus_features.csv': 'F:\\\\data\\\\LNP_AGILE\\\\data\\\\candidate_set_smiles_plus_features.csv',\n",
       " 'finetuning_set_smiles_plus_features.csv': 'F:\\\\data\\\\LNP_AGILE\\\\data\\\\finetuning_set_smiles_plus_features.csv',\n",
       " '._candidate_set_smiles_plus_features.csv': 'F:\\\\data\\\\LNP_AGILE\\\\__MACOSX\\\\data\\\\._candidate_set_smiles_plus_features.csv',\n",
       " '._finetuning_set_smiles_plus_features.csv': 'F:\\\\data\\\\LNP_AGILE\\\\__MACOSX\\\\data\\\\._finetuning_set_smiles_plus_features.csv'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tempfile\n",
    "from devkit.dataset import fetch_file, unzip_to_data_subdir, get_subdir_files_as_dict\n",
    "temp_dir = tempfile.gettempdir()\n",
    "temp_zip_path = tempfile.mktemp(suffix=\".zip\", dir=temp_dir)\n",
    "url = \"https://github.com/bowang-lab/AGILE/raw/81900a160f60f8d0728896d347fe569c952d6bd9/data.zip\"\n",
    "fetch_file(url,temp_zip_path)\n",
    "unzip_to_data_subdir(temp_zip_path,'LNP_AGILE')\n",
    "data = get_subdir_files_as_dict('LNP_AGILE')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>expt_Hela</th>\n",
       "      <th>expt_Raw</th>\n",
       "      <th>desc_ABC/10</th>\n",
       "      <th>desc_ABCGG/10</th>\n",
       "      <th>desc_nBase</th>\n",
       "      <th>desc_SpAbs_A/30</th>\n",
       "      <th>desc_SpMax_A</th>\n",
       "      <th>desc_SpDiam_A</th>\n",
       "      <th>desc_SpAD_A/30</th>\n",
       "      <th>...</th>\n",
       "      <th>desc_SRW08</th>\n",
       "      <th>desc_SRW09</th>\n",
       "      <th>desc_SRW10</th>\n",
       "      <th>desc_MW/100</th>\n",
       "      <th>desc_AMW</th>\n",
       "      <th>desc_WPath/10000</th>\n",
       "      <th>desc_WPol/10</th>\n",
       "      <th>desc_Zagreb1/100</th>\n",
       "      <th>desc_Zagreb2/100</th>\n",
       "      <th>desc_mZagreb2/10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCCCCC\\C=C/CCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCC...</td>\n",
       "      <td>0.475362</td>\n",
       "      <td>-0.943598</td>\n",
       "      <td>3.988571</td>\n",
       "      <td>2.490832</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>...</td>\n",
       "      <td>8.440096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.809891</td>\n",
       "      <td>8.027639</td>\n",
       "      <td>5.179122</td>\n",
       "      <td>2.4072</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.427778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C...</td>\n",
       "      <td>0.069628</td>\n",
       "      <td>-0.471743</td>\n",
       "      <td>3.988571</td>\n",
       "      <td>2.490832</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>...</td>\n",
       "      <td>8.440096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.809891</td>\n",
       "      <td>8.047795</td>\n",
       "      <td>5.125984</td>\n",
       "      <td>2.4072</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.427778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/CCC...</td>\n",
       "      <td>3.245195</td>\n",
       "      <td>5.423429</td>\n",
       "      <td>3.564307</td>\n",
       "      <td>2.337770</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.110146</td>\n",
       "      <td>2.200337</td>\n",
       "      <td>4.400675</td>\n",
       "      <td>2.110146</td>\n",
       "      <td>...</td>\n",
       "      <td>8.344980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.723224</td>\n",
       "      <td>7.206856</td>\n",
       "      <td>5.184789</td>\n",
       "      <td>1.7158</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/C...</td>\n",
       "      <td>2.508357</td>\n",
       "      <td>2.310532</td>\n",
       "      <td>3.705728</td>\n",
       "      <td>2.390907</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.195279</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.195279</td>\n",
       "      <td>...</td>\n",
       "      <td>8.377701</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.752955</td>\n",
       "      <td>7.487169</td>\n",
       "      <td>5.163565</td>\n",
       "      <td>1.9248</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.327778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C...</td>\n",
       "      <td>5.427294</td>\n",
       "      <td>0.771155</td>\n",
       "      <td>3.847150</td>\n",
       "      <td>2.441808</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.280361</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.280361</td>\n",
       "      <td>...</td>\n",
       "      <td>8.409385</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.781828</td>\n",
       "      <td>7.767482</td>\n",
       "      <td>5.144028</td>\n",
       "      <td>2.1550</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.377778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>CCCCCCCC\\C=C/CCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCC...</td>\n",
       "      <td>6.501837</td>\n",
       "      <td>1.705982</td>\n",
       "      <td>3.518325</td>\n",
       "      <td>2.407701</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.039306</td>\n",
       "      <td>2.233648</td>\n",
       "      <td>4.467296</td>\n",
       "      <td>2.039306</td>\n",
       "      <td>...</td>\n",
       "      <td>8.427487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.852142</td>\n",
       "      <td>7.036591</td>\n",
       "      <td>5.212290</td>\n",
       "      <td>1.5633</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>CCCCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC...</td>\n",
       "      <td>6.142469</td>\n",
       "      <td>-0.891244</td>\n",
       "      <td>3.518325</td>\n",
       "      <td>2.407701</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.039306</td>\n",
       "      <td>2.233648</td>\n",
       "      <td>4.467296</td>\n",
       "      <td>2.039306</td>\n",
       "      <td>...</td>\n",
       "      <td>8.427487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.852142</td>\n",
       "      <td>7.056747</td>\n",
       "      <td>5.150911</td>\n",
       "      <td>1.5633</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC=C)NCC...</td>\n",
       "      <td>8.177133</td>\n",
       "      <td>2.918444</td>\n",
       "      <td>3.094061</td>\n",
       "      <td>2.246607</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.784046</td>\n",
       "      <td>2.233648</td>\n",
       "      <td>4.467296</td>\n",
       "      <td>1.784046</td>\n",
       "      <td>...</td>\n",
       "      <td>8.331105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.769213</td>\n",
       "      <td>6.215808</td>\n",
       "      <td>5.223368</td>\n",
       "      <td>1.0378</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>CCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC=C)N...</td>\n",
       "      <td>-0.080869</td>\n",
       "      <td>-0.544540</td>\n",
       "      <td>3.235482</td>\n",
       "      <td>2.302550</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.869178</td>\n",
       "      <td>2.233648</td>\n",
       "      <td>4.467296</td>\n",
       "      <td>1.869178</td>\n",
       "      <td>...</td>\n",
       "      <td>8.364275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.797627</td>\n",
       "      <td>6.496121</td>\n",
       "      <td>5.196897</td>\n",
       "      <td>1.1943</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>CCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC=C...</td>\n",
       "      <td>6.261474</td>\n",
       "      <td>3.792084</td>\n",
       "      <td>3.376904</td>\n",
       "      <td>2.356126</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.954260</td>\n",
       "      <td>2.233648</td>\n",
       "      <td>4.467296</td>\n",
       "      <td>1.954260</td>\n",
       "      <td>...</td>\n",
       "      <td>8.396381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.825256</td>\n",
       "      <td>6.776434</td>\n",
       "      <td>5.172851</td>\n",
       "      <td>1.3692</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.183333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 816 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  expt_Hela  expt_Raw  \\\n",
       "0     CCCCCCCC\\C=C/CCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCC...   0.475362 -0.943598   \n",
       "1     CCCCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C...   0.069628 -0.471743   \n",
       "2     CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/CCC...   3.245195  5.423429   \n",
       "3     CCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/C...   2.508357  2.310532   \n",
       "4     CCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C...   5.427294  0.771155   \n",
       "...                                                 ...        ...       ...   \n",
       "1195  CCCCCCCC\\C=C/CCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCC...   6.501837  1.705982   \n",
       "1196  CCCCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC...   6.142469 -0.891244   \n",
       "1197  CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC=C)NCC...   8.177133  2.918444   \n",
       "1198  CCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC=C)N...  -0.080869 -0.544540   \n",
       "1199  CCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCCCC=C...   6.261474  3.792084   \n",
       "\n",
       "      desc_ABC/10  desc_ABCGG/10  desc_nBase  desc_SpAbs_A/30  desc_SpMax_A  \\\n",
       "0        3.988571       2.490832         3.0         2.365406      2.200338   \n",
       "1        3.988571       2.490832         3.0         2.365406      2.200338   \n",
       "2        3.564307       2.337770         3.0         2.110146      2.200337   \n",
       "3        3.705728       2.390907         3.0         2.195279      2.200338   \n",
       "4        3.847150       2.441808         3.0         2.280361      2.200338   \n",
       "...           ...            ...         ...              ...           ...   \n",
       "1195     3.518325       2.407701         2.0         2.039306      2.233648   \n",
       "1196     3.518325       2.407701         2.0         2.039306      2.233648   \n",
       "1197     3.094061       2.246607         2.0         1.784046      2.233648   \n",
       "1198     3.235482       2.302550         2.0         1.869178      2.233648   \n",
       "1199     3.376904       2.356126         2.0         1.954260      2.233648   \n",
       "\n",
       "      desc_SpDiam_A  desc_SpAD_A/30  ...  desc_SRW08  desc_SRW09  desc_SRW10  \\\n",
       "0          4.400676        2.365406  ...    8.440096         0.0    9.809891   \n",
       "1          4.400676        2.365406  ...    8.440096         0.0    9.809891   \n",
       "2          4.400675        2.110146  ...    8.344980         0.0    9.723224   \n",
       "3          4.400676        2.195279  ...    8.377701         0.0    9.752955   \n",
       "4          4.400676        2.280361  ...    8.409385         0.0    9.781828   \n",
       "...             ...             ...  ...         ...         ...         ...   \n",
       "1195       4.467296        2.039306  ...    8.427487         0.0    9.852142   \n",
       "1196       4.467296        2.039306  ...    8.427487         0.0    9.852142   \n",
       "1197       4.467296        1.784046  ...    8.331105         0.0    9.769213   \n",
       "1198       4.467296        1.869178  ...    8.364275         0.0    9.797627   \n",
       "1199       4.467296        1.954260  ...    8.396381         0.0    9.825256   \n",
       "\n",
       "      desc_MW/100  desc_AMW  desc_WPath/10000  desc_WPol/10  desc_Zagreb1/100  \\\n",
       "0        8.027639  5.179122            2.4072           6.0              2.30   \n",
       "1        8.047795  5.125984            2.4072           6.0              2.30   \n",
       "2        7.206856  5.184789            1.7158           5.4              2.06   \n",
       "3        7.487169  5.163565            1.9248           5.6              2.14   \n",
       "4        7.767482  5.144028            2.1550           5.8              2.22   \n",
       "...           ...       ...               ...           ...               ...   \n",
       "1195     7.036591  5.212290            1.5633           5.6              2.06   \n",
       "1196     7.056747  5.150911            1.5633           5.6              2.06   \n",
       "1197     6.215808  5.223368            1.0378           5.0              1.82   \n",
       "1198     6.496121  5.196897            1.1943           5.2              1.90   \n",
       "1199     6.776434  5.172851            1.3692           5.4              1.98   \n",
       "\n",
       "      desc_Zagreb2/100  desc_mZagreb2/10  \n",
       "0                 2.34          1.427778  \n",
       "1                 2.34          1.427778  \n",
       "2                 2.10          1.277778  \n",
       "3                 2.18          1.327778  \n",
       "4                 2.26          1.377778  \n",
       "...                ...               ...  \n",
       "1195              2.13          1.233333  \n",
       "1196              2.13          1.233333  \n",
       "1197              1.89          1.083333  \n",
       "1198              1.97          1.133333  \n",
       "1199              2.05          1.183333  \n",
       "\n",
       "[1200 rows x 816 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data['finetuning_set_smiles_plus_features.csv'])\n",
    "can_df = pd.read_csv(data['candidate_set_smiles_plus_features.csv'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devkit.utils import tqdm_imap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.kernels import Kernel\n",
    "from gpytorch.kernels.scale_kernel import ScaleKernel\n",
    "from gpytorch.distributions.multivariate_normal import MultivariateNormal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def smi_to_fingerprint(smiles,radius=2, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "        arr = np.zeros((1,), dtype=np.uint8)\n",
    "        DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        return arr\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09768935b2a4bb3ac5ba45fc98b7edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Serial f=smi_to_fingerprint:   0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fingerprints = tqdm_imap(df.smiles.to_list(),smi_to_fingerprint,n_procs=1)\n",
    "df['morgan_fp'] = fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>expt_Hela</th>\n",
       "      <th>expt_Raw</th>\n",
       "      <th>desc_ABC/10</th>\n",
       "      <th>desc_ABCGG/10</th>\n",
       "      <th>desc_nBase</th>\n",
       "      <th>desc_SpAbs_A/30</th>\n",
       "      <th>desc_SpMax_A</th>\n",
       "      <th>desc_SpDiam_A</th>\n",
       "      <th>desc_SpAD_A/30</th>\n",
       "      <th>...</th>\n",
       "      <th>desc_SRW09</th>\n",
       "      <th>desc_SRW10</th>\n",
       "      <th>desc_MW/100</th>\n",
       "      <th>desc_AMW</th>\n",
       "      <th>desc_WPath/10000</th>\n",
       "      <th>desc_WPol/10</th>\n",
       "      <th>desc_Zagreb1/100</th>\n",
       "      <th>desc_Zagreb2/100</th>\n",
       "      <th>desc_mZagreb2/10</th>\n",
       "      <th>morgan_fp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCCCCC\\C=C/CCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCC...</td>\n",
       "      <td>0.475362</td>\n",
       "      <td>-0.943598</td>\n",
       "      <td>3.988571</td>\n",
       "      <td>2.490832</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.809891</td>\n",
       "      <td>8.027639</td>\n",
       "      <td>5.179122</td>\n",
       "      <td>2.4072</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.427778</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C...</td>\n",
       "      <td>0.069628</td>\n",
       "      <td>-0.471743</td>\n",
       "      <td>3.988571</td>\n",
       "      <td>2.490832</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.365406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.809891</td>\n",
       "      <td>8.047795</td>\n",
       "      <td>5.125984</td>\n",
       "      <td>2.4072</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1.427778</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/CCC...</td>\n",
       "      <td>3.245195</td>\n",
       "      <td>5.423429</td>\n",
       "      <td>3.564307</td>\n",
       "      <td>2.337770</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.110146</td>\n",
       "      <td>2.200337</td>\n",
       "      <td>4.400675</td>\n",
       "      <td>2.110146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.723224</td>\n",
       "      <td>7.206856</td>\n",
       "      <td>5.184789</td>\n",
       "      <td>1.7158</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.277778</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/C...</td>\n",
       "      <td>2.508357</td>\n",
       "      <td>2.310532</td>\n",
       "      <td>3.705728</td>\n",
       "      <td>2.390907</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.195279</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.195279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.752955</td>\n",
       "      <td>7.487169</td>\n",
       "      <td>5.163565</td>\n",
       "      <td>1.9248</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.327778</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C...</td>\n",
       "      <td>5.427294</td>\n",
       "      <td>0.771155</td>\n",
       "      <td>3.847150</td>\n",
       "      <td>2.441808</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.280361</td>\n",
       "      <td>2.200338</td>\n",
       "      <td>4.400676</td>\n",
       "      <td>2.280361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.781828</td>\n",
       "      <td>7.767482</td>\n",
       "      <td>5.144028</td>\n",
       "      <td>2.1550</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.377778</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 817 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  expt_Hela  expt_Raw  \\\n",
       "0  CCCCCCCC\\C=C/CCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCC...   0.475362 -0.943598   \n",
       "1  CCCCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C...   0.069628 -0.471743   \n",
       "2  CCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/CCC...   3.245195  5.423429   \n",
       "3  CCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C/C...   2.508357  2.310532   \n",
       "4  CCCCCCCCCCCCCCCCNC(=O)C(CCCCCOC(=O)CCCCCCC\\C=C...   5.427294  0.771155   \n",
       "\n",
       "   desc_ABC/10  desc_ABCGG/10  desc_nBase  desc_SpAbs_A/30  desc_SpMax_A  \\\n",
       "0     3.988571       2.490832         3.0         2.365406      2.200338   \n",
       "1     3.988571       2.490832         3.0         2.365406      2.200338   \n",
       "2     3.564307       2.337770         3.0         2.110146      2.200337   \n",
       "3     3.705728       2.390907         3.0         2.195279      2.200338   \n",
       "4     3.847150       2.441808         3.0         2.280361      2.200338   \n",
       "\n",
       "   desc_SpDiam_A  desc_SpAD_A/30  ...  desc_SRW09  desc_SRW10  desc_MW/100  \\\n",
       "0       4.400676        2.365406  ...         0.0    9.809891     8.027639   \n",
       "1       4.400676        2.365406  ...         0.0    9.809891     8.047795   \n",
       "2       4.400675        2.110146  ...         0.0    9.723224     7.206856   \n",
       "3       4.400676        2.195279  ...         0.0    9.752955     7.487169   \n",
       "4       4.400676        2.280361  ...         0.0    9.781828     7.767482   \n",
       "\n",
       "   desc_AMW  desc_WPath/10000  desc_WPol/10  desc_Zagreb1/100  \\\n",
       "0  5.179122            2.4072           6.0              2.30   \n",
       "1  5.125984            2.4072           6.0              2.30   \n",
       "2  5.184789            1.7158           5.4              2.06   \n",
       "3  5.163565            1.9248           5.6              2.14   \n",
       "4  5.144028            2.1550           5.8              2.22   \n",
       "\n",
       "   desc_Zagreb2/100  desc_mZagreb2/10  \\\n",
       "0              2.34          1.427778   \n",
       "1              2.34          1.427778   \n",
       "2              2.10          1.277778   \n",
       "3              2.18          1.327778   \n",
       "4              2.26          1.377778   \n",
       "\n",
       "                                           morgan_fp  \n",
       "0  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "\n",
       "[5 rows x 817 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['expt_Hela'].to_numpy()\n",
    "X = np.vstack(df['morgan_fp'].to_list())\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train).double()\n",
    "X_test = torch.tensor(X_test).double()\n",
    "y_train = torch.tensor(y_train).double()\n",
    "y_test = torch.tensor(y_test).double()\n",
    "X_train.dim(),X_test.dim(),y_train.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.]], dtype=torch.float64),\n",
       " tensor([ 7.3466e+00,  8.1873e-01,  3.0482e-01,  7.5050e+00,  1.0020e+00,\n",
       "          4.6565e+00,  1.5100e+00,  7.4190e+00,  4.7993e-01,  2.9523e+00,\n",
       "          7.9996e-01,  5.8478e+00,  5.5398e+00,  2.7007e+00,  4.4703e+00,\n",
       "          5.6291e+00,  1.5317e+00,  1.1901e+00,  6.6852e+00,  5.3890e+00,\n",
       "          7.0214e+00,  4.4211e+00,  1.6712e+00,  3.0706e+00,  9.4249e+00,\n",
       "          6.2564e+00,  4.5834e+00,  6.8632e+00,  4.4018e-01,  7.1215e+00,\n",
       "          7.1357e+00,  4.4335e+00,  5.8719e+00,  4.1337e+00,  6.9817e+00,\n",
       "          2.7007e+00,  5.9417e+00,  5.9980e+00,  8.1186e+00, -1.3898e-01,\n",
       "         -1.0031e-02,  5.6796e+00,  2.5084e+00,  3.8408e+00,  2.4043e+00,\n",
       "          5.2973e+00,  4.9955e+00,  6.5308e+00,  7.2019e-01,  7.2074e+00,\n",
       "          3.7405e+00,  6.7994e+00,  1.4609e+01,  5.5152e+00,  4.9512e+00,\n",
       "          5.1717e+00,  8.1398e+00,  6.0359e+00,  1.5477e+00,  6.9499e+00,\n",
       "          8.5533e+00,  6.3965e+00,  8.3192e+00,  7.3640e+00,  2.5439e+00,\n",
       "          8.1134e+00,  7.6033e-01,  9.7877e+00,  9.2488e+00,  1.0696e+00,\n",
       "          5.0315e-01, -1.3915e-01,  8.6449e+00,  5.9203e+00,  6.1985e+00,\n",
       "          2.5605e+00,  5.2663e+00,  6.3257e+00,  3.3279e+00,  5.4878e+00,\n",
       "          3.2766e+00,  1.9991e+00,  4.2603e+00,  5.0883e+00,  1.8552e+00,\n",
       "          4.4484e+00,  1.2241e+01,  5.4454e+00,  6.1011e+00,  1.2993e+01,\n",
       "          1.6133e+00,  3.2452e+00, -2.6193e-01,  3.2526e+00,  8.6239e+00,\n",
       "          7.6779e+00,  8.2174e+00,  2.0055e+00,  9.5100e+00,  1.3042e+00,\n",
       "          7.3779e+00,  5.6876e+00,  4.2313e+00,  6.4793e+00,  2.1063e+00,\n",
       "          4.4973e+00,  6.6274e+00,  8.5634e+00,  5.4347e+00, -6.3775e-01,\n",
       "          5.4796e+00,  6.8840e+00,  2.0281e+00,  6.0289e+00,  5.2548e+00,\n",
       "          1.9017e+00, -1.3936e+00,  5.2087e-01,  1.0605e+01,  2.2239e-01,\n",
       "         -3.9070e-02,  1.2683e+00,  1.0045e+01,  9.5135e-01,  7.6459e+00,\n",
       "          9.0982e-01,  3.8394e+00,  4.8648e+00,  5.0044e+00,  1.6670e+00,\n",
       "          6.7604e+00,  5.7732e+00,  9.9558e+00,  1.7642e+00,  5.2126e+00,\n",
       "          9.5693e-01,  1.7226e+00,  8.9968e+00,  4.2723e+00,  1.8010e+00,\n",
       "          3.8241e-01,  6.9821e+00,  6.8149e+00,  5.3244e+00,  4.8087e+00,\n",
       "          8.4931e-01,  5.8894e-02,  3.3818e+00,  6.2518e+00,  8.8922e+00,\n",
       "          1.3836e+00,  5.1326e+00,  4.7337e+00,  3.5103e+00,  7.4547e+00,\n",
       "          7.7502e-01,  5.3330e+00,  2.3749e+00,  4.5313e+00,  6.2360e+00,\n",
       "          8.5720e+00,  7.7626e+00,  3.9175e+00,  7.7483e+00,  3.1119e+00,\n",
       "          2.2030e+00,  7.9925e+00,  9.4864e+00,  2.3865e+00,  3.0649e+00,\n",
       "          6.2828e+00,  8.7232e+00,  3.6521e+00,  5.7045e+00,  8.1499e+00,\n",
       "          1.2930e+00,  6.2677e+00,  1.3921e+01,  5.1153e+00,  7.6064e+00,\n",
       "          5.1635e+00,  1.3624e+00,  6.5692e+00,  3.5418e+00,  4.7362e+00,\n",
       "          3.8495e+00,  3.9730e+00,  7.1433e+00,  4.1846e+00,  1.0682e+00,\n",
       "          6.2819e+00,  3.3532e+00,  7.7718e+00, -1.6648e-01,  6.6646e+00,\n",
       "          7.6113e+00,  1.2524e+00,  8.7171e+00,  3.8687e+00,  2.4094e+00,\n",
       "          8.0826e+00,  5.3797e+00,  1.3936e+00,  5.2479e+00,  9.2488e-01,\n",
       "          1.6403e+00, -5.8404e-01,  6.3772e+00,  2.6054e+00,  1.0550e+01,\n",
       "          1.2763e+01,  5.8925e-01,  6.8130e+00,  5.9391e+00,  8.1714e+00,\n",
       "          9.2803e-01,  5.7206e+00,  7.0943e+00,  4.1300e+00, -2.3012e+00,\n",
       "          5.1894e+00,  8.7907e+00,  7.4152e+00,  1.8301e+00,  1.7119e+00,\n",
       "          5.9248e+00,  4.4903e+00,  1.7070e+00,  7.4947e-01,  6.8261e+00,\n",
       "          6.3232e+00, -1.6389e-02,  8.6872e+00,  5.5689e+00,  5.3764e+00,\n",
       "          5.2487e+00, -3.7004e-01,  1.3006e+00,  7.1816e+00,  9.6078e+00,\n",
       "          5.5838e+00,  6.5358e+00,  7.0418e+00,  1.1121e+01,  5.3295e+00,\n",
       "          9.8377e-01,  7.2819e+00,  4.8596e+00,  8.4329e+00,  6.2336e+00,\n",
       "          5.8987e-01,  3.5530e+00,  3.5018e+00,  7.3006e+00,  4.6430e+00,\n",
       "          8.3403e+00,  4.2345e+00,  3.0262e+00,  8.7387e+00,  7.7884e+00,\n",
       "          1.3585e+01,  1.3213e+01,  9.5149e-01,  6.5135e+00,  6.9444e+00,\n",
       "          7.4788e+00,  1.0686e+00,  8.0992e+00,  7.7567e+00,  2.3589e+00,\n",
       "          1.0163e+00,  2.3002e+00,  4.7187e-02,  6.5018e+00,  5.6320e+00,\n",
       "          8.2631e+00,  5.4273e+00,  3.4135e+00,  1.8461e+00,  2.5284e-01,\n",
       "          7.0176e+00,  7.7901e+00,  1.2432e+01,  4.0722e-01,  3.6286e-01,\n",
       "          4.9856e-03,  6.8866e+00,  1.3181e+00,  3.9627e-01,  8.9969e+00,\n",
       "          4.9540e+00,  8.5471e-01,  6.6855e+00,  7.4851e+00,  6.0303e+00,\n",
       "          6.3083e+00,  7.9906e+00,  7.2515e+00,  9.1463e+00,  4.8393e-01,\n",
       "         -1.7383e-02,  3.8285e+00,  9.5483e-01,  7.1607e+00,  6.8266e+00,\n",
       "          7.9949e+00,  4.0665e+00,  2.6399e-01,  4.6201e+00,  4.8187e+00,\n",
       "          7.3598e+00,  3.6602e+00,  3.7485e+00,  5.0686e+00,  7.3177e+00,\n",
       "          8.7468e+00,  5.7305e+00,  7.8688e+00,  2.1552e-01,  7.2457e+00,\n",
       "          1.7046e+00,  9.8016e+00,  7.0285e+00,  7.0847e+00,  8.6297e+00,\n",
       "          6.8781e+00,  5.9215e+00,  7.9011e+00,  8.4064e+00,  1.3490e+00,\n",
       "          7.8543e-01,  1.4793e+00,  5.3374e+00,  4.1207e+00,  7.0503e+00,\n",
       "          9.9303e+00,  2.3299e+00,  3.7299e-01,  5.9856e+00,  1.5471e+00,\n",
       "          7.3899e+00,  2.4020e-01,  8.1209e-02,  4.9190e+00,  6.1122e+00,\n",
       "          4.7536e-01,  7.2320e+00,  4.4848e+00,  7.5281e+00,  1.8214e+00,\n",
       "          7.5977e+00,  2.9644e-01,  8.3656e+00,  9.9150e-01,  6.8410e+00,\n",
       "          6.7753e+00,  1.8908e+00,  5.9669e+00,  2.7587e+00,  6.1038e+00,\n",
       "          5.0816e+00,  7.4992e+00,  3.5957e+00,  2.6194e+00,  1.1626e-01,\n",
       "          2.9729e+00,  7.0736e+00,  7.1559e-01,  6.7584e+00,  1.5960e+01,\n",
       "          2.4023e-01,  8.6347e+00,  7.8208e+00,  5.3808e+00,  8.8667e-01,\n",
       "          5.6284e-01, -3.9833e-01, -1.7834e-01, -2.9528e-01,  3.5957e+00,\n",
       "          7.4167e+00, -3.1112e-01, -1.5983e+00,  7.2494e+00,  1.8238e+00,\n",
       "          8.2585e+00, -1.2292e+00,  8.5658e+00,  6.7194e+00,  5.1217e+00,\n",
       "          1.6404e+00,  6.1240e+00,  9.3892e+00,  6.9634e+00,  6.3921e+00,\n",
       "          7.4456e+00,  2.3219e+00,  3.4054e+00,  6.8581e-01,  7.2419e+00,\n",
       "          5.8158e+00,  1.1275e+01,  1.5368e+00,  6.9976e+00,  6.4741e+00,\n",
       "          6.6306e+00,  6.4597e+00,  2.8989e+00,  5.3915e+00,  6.6208e+00,\n",
       "          8.6012e+00,  5.6306e+00,  7.4026e+00,  2.9567e+00,  5.3276e+00,\n",
       "          2.8752e+00,  5.6088e+00,  3.3219e+00,  5.6500e+00,  3.4093e+00,\n",
       "          6.9144e+00,  9.9520e+00,  1.2296e+00,  1.5187e+00,  5.8641e+00,\n",
       "          1.1257e+00,  7.3819e+00, -4.7617e-01,  5.0376e+00,  8.3050e+00,\n",
       "          4.5065e+00,  8.0326e+00,  5.5337e+00,  8.8754e+00,  1.8637e-01,\n",
       "          6.7891e+00,  8.6058e+00,  9.3355e+00,  2.7097e+00,  7.5153e+00,\n",
       "          5.4372e+00,  6.6177e+00,  7.6081e+00,  5.7814e+00,  1.5550e-01,\n",
       "          4.6725e+00,  7.3678e+00,  1.7160e+00, -1.0677e+00,  5.6229e+00,\n",
       "          7.1636e+00,  7.8480e-02,  6.8764e+00,  2.0113e+00,  2.8020e+00,\n",
       "          7.7771e-01,  8.8083e-01,  2.4296e+00,  1.1878e+00,  6.3034e+00,\n",
       "          8.1574e+00,  8.2753e+00,  5.4466e+00,  7.8904e+00, -5.6527e-01,\n",
       "          6.9382e+00,  4.8157e+00,  6.4992e+00,  9.4392e+00,  1.6066e+00,\n",
       "          2.8727e+00,  3.9568e+00,  7.3978e+00,  1.0068e+00,  7.9997e+00,\n",
       "          6.4263e+00,  2.0024e+00, -1.8635e+00,  2.6353e+00,  5.5061e+00,\n",
       "          2.0381e+00,  5.7599e+00,  3.1138e+00,  2.7937e+00,  6.8759e+00,\n",
       "          1.3075e+00,  5.7278e-01,  8.4156e-01,  2.2441e+00,  9.2207e-01,\n",
       "          5.7963e+00,  1.2969e+01,  2.9157e+00,  7.8543e-01, -6.9292e-01,\n",
       "          1.8970e+00,  4.9604e+00,  5.7494e+00,  8.8877e+00,  8.0164e+00,\n",
       "          1.7652e+00,  2.9241e+00,  1.7332e+00,  6.6871e+00,  5.7220e+00,\n",
       "          4.8364e+00,  2.5429e-01,  4.0434e-01,  1.8052e+00,  9.9724e-01,\n",
       "          1.0664e+00,  6.9925e-01,  1.1057e+01,  7.5222e+00,  5.2596e+00,\n",
       "          2.8710e+00,  2.4811e+00,  3.9550e+00,  7.8338e+00,  1.1027e+01,\n",
       "          1.2730e+00,  3.8257e+00,  7.8625e+00,  1.0090e+01,  8.4684e-01,\n",
       "          1.7471e+00,  8.3180e+00,  5.0187e+00,  8.0632e+00,  5.0291e+00,\n",
       "          4.2923e+00,  1.3775e+00,  3.3394e+00,  4.0939e+00,  1.1143e+01,\n",
       "          1.3872e+00,  6.9650e+00,  1.5083e+01,  1.3377e+00,  3.9065e+00,\n",
       "          1.2047e+00,  3.2217e+00,  5.8592e+00,  1.2825e+01,  7.2251e+00,\n",
       "          9.4903e-01,  2.7210e+00,  1.3853e+01,  8.9185e+00,  3.4175e+00,\n",
       "          4.3762e-01,  6.0346e+00,  9.1045e-01,  5.9503e+00,  5.6036e+00,\n",
       "          5.3594e+00,  3.7504e+00,  7.8924e-01,  3.9306e+00,  3.6441e+00,\n",
       "         -2.3456e+00,  2.6505e+00,  5.4273e+00,  1.4342e+00,  7.2893e+00,\n",
       "          8.7581e+00,  3.6081e+00,  8.7452e+00,  6.2300e+00,  6.1815e+00,\n",
       "          2.2783e+00,  7.0573e+00,  5.6632e+00,  7.5857e-01,  7.6633e+00,\n",
       "          7.1049e+00,  1.1440e+00,  1.3292e+00,  6.4597e+00,  7.8090e+00,\n",
       "          5.5782e+00,  1.3580e+00,  2.4264e+00,  3.8195e+00,  1.0051e+01,\n",
       "          8.0094e+00,  6.0247e+00,  2.7650e+00,  9.7656e+00,  1.0104e+01,\n",
       "          6.3655e+00,  6.7858e+00,  4.6551e+00,  2.5192e+00,  7.2628e+00,\n",
       "          2.9500e+00,  2.3354e+00, -2.8006e-01, -8.2023e-01,  6.0803e-01,\n",
       "          4.6879e+00,  7.4828e+00,  1.0999e+00,  1.5961e+00,  3.9295e+00,\n",
       "          3.1442e+00,  6.6677e+00,  7.6457e+00,  1.9832e+00,  1.1964e+00,\n",
       "          6.3994e+00,  2.2318e+00,  5.0993e+00,  7.8816e+00,  2.6055e+00,\n",
       "          4.0581e+00,  7.0396e+00,  8.7298e+00,  2.9297e+00,  1.5683e+00,\n",
       "          5.6545e+00,  1.0100e+01,  7.1106e+00,  6.2615e+00,  5.7671e+00,\n",
       "          7.4451e+00,  4.2919e+00,  7.1695e+00,  5.9831e+00,  8.2677e+00,\n",
       "          9.4495e-01,  1.5005e+00,  1.2681e+00,  7.9231e+00,  3.8433e+00,\n",
       "          6.0185e+00, -2.4218e-01,  2.4658e+00,  6.2725e+00,  7.7561e+00,\n",
       "          1.9886e+00,  5.3351e+00,  1.5086e+00,  6.6205e+00,  7.0970e+00,\n",
       "          1.2912e+01,  5.9905e+00,  6.8719e+00,  5.4959e+00,  8.5731e-01,\n",
       "          7.5337e+00,  2.3058e-01,  8.1722e+00,  6.5727e+00,  6.3591e+00,\n",
       "         -6.7078e-01,  7.0765e+00,  3.1577e-01,  1.3516e+00,  2.7753e+00,\n",
       "          6.4520e+00,  1.7253e+00,  7.5776e+00,  6.0210e+00,  7.4567e+00,\n",
       "          4.8433e+00,  9.8892e+00,  7.0095e+00,  4.4150e+00,  6.2000e+00,\n",
       "          6.9794e+00,  3.3716e+00,  1.1148e+01,  1.1289e+00,  7.0252e+00,\n",
       "          4.8243e+00,  9.2581e+00,  6.7896e+00,  5.8730e+00,  6.1019e+00,\n",
       "          2.4823e+00,  1.5327e+01,  3.2032e+00,  2.5619e+00,  5.9642e+00,\n",
       "          6.9633e+00,  6.4056e+00,  6.8719e+00,  1.1523e+00,  9.0368e+00,\n",
       "          9.4980e-01,  6.2232e+00,  1.3006e+00,  2.6015e+00,  1.4014e+00,\n",
       "          6.3947e+00,  1.8702e+00,  3.2503e+00,  5.5907e+00,  4.5628e+00,\n",
       "          8.1166e+00,  1.1695e+00,  3.9486e+00,  2.0444e+00,  7.4141e+00,\n",
       "          8.1771e+00,  2.0312e-01,  7.2109e+00,  1.5460e+00, -5.1537e-01,\n",
       "          6.7937e+00,  7.8821e+00,  1.2387e+00, -8.0869e-02,  6.6190e+00,\n",
       "          5.3310e+00,  4.2595e-01,  9.5328e-01,  7.3864e-01,  7.7767e+00,\n",
       "          7.7117e+00,  7.8336e+00,  8.5678e-01,  6.9155e+00,  1.5758e+00,\n",
       "          2.5616e+00,  6.9514e+00,  7.5520e+00,  3.3476e+00,  2.0661e+00,\n",
       "          2.9500e+00,  1.1340e+01,  5.1060e+00,  1.0202e+00,  7.8915e+00,\n",
       "          1.1488e+00,  1.7917e+00,  1.9409e+00,  5.7625e+00,  6.9328e+00,\n",
       "         -2.6515e-01,  1.1192e-01,  1.0174e+00,  4.1718e+00,  1.0593e+01,\n",
       "          8.3219e+00, -6.7879e-01,  5.8114e+00,  7.3893e+00,  9.2634e+00,\n",
       "          5.3807e+00,  3.9516e+00,  1.5046e+00,  7.8229e+00,  1.1269e+01,\n",
       "          5.5463e+00,  3.2185e+00,  7.2404e+00,  6.2458e+00,  2.1916e+00,\n",
       "          6.5208e+00,  7.9079e+00,  7.5300e+00,  4.8693e+00,  8.3098e+00,\n",
       "          2.8582e+00,  1.2202e+01,  6.6604e+00,  2.9645e+00,  6.7287e+00,\n",
       "          1.1152e+00,  1.0236e+01,  6.7923e+00,  8.6564e+00, -3.2293e-01,\n",
       "          5.0566e+00,  8.9589e+00,  1.1601e+00,  4.2849e-01,  4.8271e+00,\n",
       "          7.3676e+00,  7.8892e+00,  2.1363e-01,  5.8128e+00,  1.8071e+00,\n",
       "          4.3546e-01,  3.9286e+00,  6.0391e+00,  1.6596e+00,  1.1753e+00,\n",
       "          1.0901e+01,  1.0638e+01,  5.7076e+00,  9.1508e-01,  5.5889e+00,\n",
       "          6.2332e+00,  8.1343e+00,  8.2520e+00,  9.2649e-01,  5.6324e+00,\n",
       "          9.5528e-01,  1.1736e+01,  7.6974e+00,  6.5676e+00,  3.8987e+00,\n",
       "          1.6145e+00,  1.0791e+00,  1.0647e+01,  6.9628e-02,  1.9130e+00,\n",
       "          2.0253e+00,  6.0723e+00,  6.9987e+00,  6.8652e+00,  1.7083e+00,\n",
       "          6.1425e+00,  6.1059e-01,  2.8792e+00,  4.7445e-01,  7.2731e+00,\n",
       "          3.1552e+00,  7.3356e+00,  4.5984e+00,  3.1832e+00,  9.2392e+00,\n",
       "          6.4190e+00,  4.5397e-01,  4.2692e+00,  3.9424e+00,  6.6046e+00,\n",
       "         -1.9355e-01,  6.7295e+00,  7.0558e+00,  1.2074e+00,  1.9027e+00,\n",
       "          5.0276e+00,  1.5236e+00,  7.7804e+00,  5.0764e+00,  6.6842e+00,\n",
       "          5.5895e+00,  7.6746e+00,  9.6334e+00,  3.0033e+00,  8.3179e+00,\n",
       "          1.2137e+01,  6.7358e+00,  8.5423e+00,  5.7221e+00,  6.0893e+00,\n",
       "          7.2011e+00,  5.8906e+00,  3.1032e+00,  7.4959e+00,  1.3027e+01,\n",
       "          4.6013e+00, -5.7833e-01,  1.2876e+00,  4.9958e+00,  1.2173e-01,\n",
       "          6.5789e+00,  6.7174e+00,  4.2538e+00,  4.8375e+00, -1.5516e-01,\n",
       "          8.3419e+00,  3.4570e+00,  1.3150e+01,  1.4035e+00,  4.4610e+00,\n",
       "          7.2407e+00,  7.5618e+00,  3.4067e-01,  6.7728e+00,  6.1719e+00,\n",
       "          6.9567e+00,  3.4847e+00,  6.9315e+00,  8.8667e-01,  2.8020e+00,\n",
       "          6.8632e+00,  8.8677e+00,  5.8262e+00,  5.3135e+00,  2.3397e+00,\n",
       "         -1.4621e+00,  1.2045e+01,  6.4465e+00,  8.1174e+00,  4.2207e+00,\n",
       "          2.3081e+00,  9.7748e+00,  6.1880e-01,  7.6236e+00,  1.2504e+00,\n",
       "          6.6930e+00,  5.9099e+00, -4.0727e-01,  1.4746e-01,  6.1031e+00,\n",
       "         -3.6231e-01,  4.2258e+00,  4.8097e+00,  5.9464e+00,  5.4808e+00,\n",
       "          9.9797e+00,  5.7798e-01,  1.7114e+00,  8.3419e+00,  4.2792e+00,\n",
       "          1.2839e+00,  5.6945e+00,  2.4592e+00,  7.3307e+00,  5.3694e+00,\n",
       "          4.7881e+00,  6.6966e+00,  7.4905e-01,  1.7769e+00,  3.0215e+00,\n",
       "          4.4249e+00,  1.1347e+00,  8.1273e+00,  1.2609e+01,  6.2629e+00,\n",
       "          3.3846e-02,  7.0161e+00,  6.6900e+00,  1.0386e+01,  5.3052e+00,\n",
       "          7.3643e+00,  1.1878e+00,  6.4724e+00,  3.7442e+00,  1.2488e+00,\n",
       "          4.9016e+00,  8.7499e+00,  6.6991e+00,  3.6425e+00,  8.3134e+00,\n",
       "         -3.9979e-01,  6.5420e+00,  6.4287e+00,  6.3517e+00,  8.8556e+00,\n",
       "          1.6079e+00,  6.1008e+00,  5.8802e+00,  6.4734e+00,  6.6096e+00,\n",
       "          5.1556e+00,  5.9174e+00,  6.2361e+00,  7.0461e+00,  4.0816e-01,\n",
       "          3.3545e+00,  8.2773e+00,  7.6899e+00,  8.0300e+00,  6.6748e+00],\n",
       "        dtype=torch.float64),\n",
       " tensor([ 6.1385,  7.9780, -0.8979,  9.8162,  0.9905,  7.5680,  5.7456,  6.2610,\n",
       "          1.1587,  8.2200,  7.7123,  9.1463,  1.3292,  4.8459,  9.2034,  0.0589,\n",
       "          4.2675,  6.7599,  9.7243,  5.1211,  1.2155,  2.0304,  5.1020,  2.2796,\n",
       "          0.8547,  6.6595,  2.0695,  7.3531,  6.8327,  7.8604,  7.0795,  8.5897,\n",
       "          2.1487,  4.8718,  6.8246,  5.4529,  5.7585,  0.1155,  3.5753,  3.8105,\n",
       "          7.1603, -1.5805,  0.8761,  7.4360,  5.5294,  4.5769,  2.7466,  1.2901,\n",
       "         10.5900,  6.1438,  7.5707,  7.6060,  7.5151,  0.9141,  2.1016,  2.7963,\n",
       "          5.9843,  6.7566, -0.6813,  7.6481,  1.7461,  1.1155,  1.1943,  2.9565,\n",
       "          0.4572,  6.5010,  2.5836, 11.0567,  2.7919,  8.4965, 11.5532,  0.0179,\n",
       "          4.3612,  8.7298,  1.2646,  8.7112,  3.5018,  0.4276,  2.3416,  5.0414,\n",
       "          6.0118,  0.9674,  3.6486,  0.2636,  5.4322,  1.3788,  4.9918, -1.0677,\n",
       "          2.6230,  0.5315,  2.0357,  8.1207,  7.4617,  7.1051,  0.7075,  3.5392,\n",
       "          1.4686,  1.8556,  8.0657,  6.7857,  5.2485,  6.7002, 14.5055,  1.7793,\n",
       "          6.9315,  8.8057, -0.2403,  1.3886,  5.1916,  6.3136,  6.1917,  8.7849,\n",
       "          0.9897,  6.5210,  8.6163,  6.9213,  3.8239, -0.8550,  7.9765,  0.8704,\n",
       "          2.3223,  1.3215,  0.9052,  6.1848,  2.1882,  6.5082,  4.6421,  3.1455,\n",
       "          5.8000, 11.1642,  8.9475,  2.6363,  0.7018,  6.8479,  5.0350,  4.0567,\n",
       "          7.6350,  6.2375,  8.6575,  8.3608,  8.3583,  4.3738,  6.5891,  3.3218,\n",
       "          3.9254,  1.4664,  2.3657, 15.0610,  7.8383,  9.5023,  0.5767, -0.3560,\n",
       "          4.7427,  1.6401,  4.7003,  5.6276,  7.3520,  6.1632,  0.6142,  1.1872,\n",
       "          6.2220,  5.0489,  2.9074,  6.5118,  8.4064,  0.4726,  7.5352,  8.2349,\n",
       "         -0.2005,  1.4431,  7.4681,  5.1450,  1.2207, -0.9411, -0.6929,  7.2352,\n",
       "          1.6736,  6.9910,  1.2797,  9.5457,  3.5258,  6.0772,  1.9061,  7.7505,\n",
       "          6.5521,  6.9754,  7.7804,  2.9126,  7.4104,  0.5839,  8.4898,  5.9505,\n",
       "          0.1497,  5.5286,  8.2608, -0.7801,  0.1119,  1.5573,  4.8763,  7.2240,\n",
       "          0.6750,  0.5955,  0.7090,  6.0899, 10.4796,  6.9478, 12.8523,  4.5459,\n",
       "          5.4009,  1.1142,  8.0883, 13.9162,  1.9071,  7.8819,  0.8957,  7.4315,\n",
       "          9.0991,  7.7275,  4.7947,  6.5363,  4.7322,  7.6611,  1.6166,  3.0706,\n",
       "          6.7552,  0.5897,  5.9801,  2.9181,  7.3281,  9.6337,  4.6671,  1.0682,\n",
       "          0.8436,  6.1186,  6.9531,  4.4862,  8.2042,  2.8042,  6.9562,  6.4968],\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test,X_train,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TanimotoKernel(Kernel):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #self.register_parameter(name=\"lengthscale\", parameter=torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x1, x2, **params):\n",
    "        x1_norm = x1.norm(dim=-1)\n",
    "        x2_norm = x2.norm(dim=-1)\n",
    "        numerator = torch.einsum(\"bi,bj->bij\", x1, x2)\n",
    "        denominator = x1_norm.unsqueeze(-1) * x2_norm.unsqueeze(-2) + 1e-8\n",
    "        return torch.exp(-self.lengthscale * (1 - numerator / denominator))\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        #self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(TanimotoKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X_train, y_train, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 32212254720 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mmll\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m   lengthscale: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m   noise: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m     23\u001b[0m     i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, training_iter, loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m.\u001b[39mcovar_module\u001b[38;5;241m.\u001b[39mbase_kernel\u001b[38;5;241m.\u001b[39mlengthscale\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m.\u001b[39mlikelihood\u001b[38;5;241m.\u001b[39mnoise\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     26\u001b[0m ))\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:64\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[0;32m     63\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 64\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:171\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mSee :py:meth:`torch.distributions.Distribution.log_prob\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m<torch.distributions.distribution.Distribution.log_prob>`.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mfast_computations\u001b[38;5;241m.\u001b[39mlog_prob\u001b[38;5;241m.\u001b[39moff():\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py:248\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m    247\u001b[0m diff \u001b[38;5;241m=\u001b[39m value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\n\u001b[1;32m--> 248\u001b[0m M \u001b[38;5;241m=\u001b[39m _batch_mahalanobis(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unbroadcasted_scale_tril\u001b[49m, diff)\n\u001b[0;32m    249\u001b[0m half_log_det \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril\u001b[38;5;241m.\u001b[39mdiagonal(dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    251\u001b[0m )\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m+\u001b[39m M) \u001b[38;5;241m-\u001b[39m half_log_det\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:88\u001b[0m, in \u001b[0;36mMultivariateNormal._unbroadcasted_scale_tril\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unbroadcasted_scale_tril\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mislazy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__unbroadcasted_scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;66;03m# cache root decoposition\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m         ust \u001b[38;5;241m=\u001b[39m to_dense(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_covariance_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m ust\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__unbroadcasted_scale_tril\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1303\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \n\u001b[0;32m   1300\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m   1305\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:510\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeops_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KeOpsLinearOperator\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtriangular_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TriangularLinearOperator\n\u001b[1;32m--> 510\u001b[0m evaluated_kern_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub_mat, KeOpsLinearOperator) \u001b[38;5;28;01mfor\u001b[39;00m sub_mat \u001b[38;5;129;01min\u001b[39;00m evaluated_kern_mat\u001b[38;5;241m.\u001b[39m_args):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run Cholesky with KeOps: it will either be really slow or not work.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\linear_operator\\operators\\added_diag_linear_operator.py:209\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 209\u001b[0m     added_diag_linear_op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_linear_op \u001b[38;5;241m+\u001b[39m added_diag_linear_op\u001b[38;5;241m.\u001b[39m_diag_tensor\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2064\u001b[0m, in \u001b[0;36mLinearOperator.representation_tree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepresentation_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LinearOperatorRepresentationTree:\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;124;03m    Returns a\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m \u001b[38;5;124;03m    :obj:`linear_operator.operators.LinearOperatorRepresentationTree` tree\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[38;5;124;03m    including all subobjects. This is used internally.\u001b[39;00m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLinearOperatorRepresentationTree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\linear_operator\\operators\\linear_operator_representation_tree.py:15\u001b[0m, in \u001b[0;36mLinearOperatorRepresentationTree.__init__\u001b[1;34m(self, linear_op)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(linear_op\u001b[38;5;241m.\u001b[39m_args, linear_op\u001b[38;5;241m.\u001b[39m_differentiable_kwargs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentation\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(arg\u001b[38;5;241m.\u001b[39mrepresentation):  \u001b[38;5;66;03m# Is it a lazy tensor?\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         representation_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mslice\u001b[39m(counter, counter \u001b[38;5;241m+\u001b[39m representation_size, \u001b[38;5;28;01mNone\u001b[39;00m), arg\u001b[38;5;241m.\u001b[39mrepresentation_tree()))\n\u001b[0;32m     17\u001b[0m         counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m representation_size\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:397\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.representation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrepresentation()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# Otherwise, we'll evaluate the kernel (or at least its LinearOperator representation) and use its\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# representation\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepresentation()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\n\u001b[0;32m    356\u001b[0m         x1,\n\u001b[0;32m    357\u001b[0m         x2,\n\u001b[0;32m    358\u001b[0m         diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m         last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dim_is_batch,\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[1;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_kernel\u001b[38;5;241m.\u001b[39mforward(x1, x2, diag\u001b[38;5;241m=\u001b[39mdiag, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mTanimotoKernel.forward\u001b[1;34m(self, x1, x2, **params)\u001b[0m\n\u001b[0;32m      7\u001b[0m x1_norm \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m x2_norm \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m numerator \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbi,bj->bij\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m denominator \u001b[38;5;241m=\u001b[39m x1_norm\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m x2_norm\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m numerator \u001b[38;5;241m/\u001b[39m denominator))\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 32212254720 bytes."
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(X_train)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, y_train)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript b has size 1200 for operand 1 which does not broadcast with previously seen size 240",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Test points are regularly spaced along [0,1]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Make predictions by feeding model through likelihood\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), gpytorch\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mfast_pred_var():\n\u001b[1;32m----> 8\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     observed_pred \u001b[38;5;241m=\u001b[39m likelihood(g)\n\u001b[0;32m     10\u001b[0m observed_pred\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\models\\exact_gp.py:333\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Make the prediction\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mcg_tolerance(settings\u001b[38;5;241m.\u001b[39meval_cg_tolerance\u001b[38;5;241m.\u001b[39mvalue()):\n\u001b[0;32m    330\u001b[0m     (\n\u001b[0;32m    331\u001b[0m         predictive_mean,\n\u001b[0;32m    332\u001b[0m         predictive_covar,\n\u001b[1;32m--> 333\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexact_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Reshape predictive mean to match the appropriate event shape\u001b[39;00m\n\u001b[0;32m    336\u001b[0m predictive_mean \u001b[38;5;241m=\u001b[39m predictive_mean\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtest_shape)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\models\\exact_prediction_strategies.py:281\u001b[0m, in \u001b[0;36mDefaultPredictionStrategy.exact_prediction\u001b[1;34m(self, joint_mean, joint_covar)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# For efficiency - we can make things more efficient\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m joint_covar\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mmax_eager_kernel_size\u001b[38;5;241m.\u001b[39mvalue():\n\u001b[1;32m--> 281\u001b[0m     test_covar \u001b[38;5;241m=\u001b[39m \u001b[43mjoint_covar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     test_test_covar \u001b[38;5;241m=\u001b[39m test_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train :]\n\u001b[0;32m    283\u001b[0m     test_train_covar \u001b[38;5;241m=\u001b[39m test_covar[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train]\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:410\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.to_dense\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\n\u001b[0;32m    356\u001b[0m         x1,\n\u001b[0;32m    357\u001b[0m         x2,\n\u001b[0;32m    358\u001b[0m         diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m         last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dim_is_batch,\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\gpytorch\\kernels\\scale_kernel.py:109\u001b[0m, in \u001b[0;36mScaleKernel.forward\u001b[1;34m(self, x1, x2, last_dim_is_batch, diag, **params)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m--> 109\u001b[0m     orig_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_kernel\u001b[38;5;241m.\u001b[39mforward(x1, x2, diag\u001b[38;5;241m=\u001b[39mdiag, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    110\u001b[0m     outputscales \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputscale\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_dim_is_batch:\n",
      "Cell \u001b[1;32mIn[30], line 17\u001b[0m, in \u001b[0;36mTanimotoKernel.forward\u001b[1;34m(self, x1, x2, **params)\u001b[0m\n\u001b[0;32m     15\u001b[0m x1_norm \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m x2_norm \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m numerator \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbi,bj->bij\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m denominator \u001b[38;5;241m=\u001b[39m x1_norm\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m x2_norm\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m numerator \u001b[38;5;241m/\u001b[39m denominator))\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\n8dev\\lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): subscript b has size 1200 for operand 1 which does not broadcast with previously seen size 240"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    g = model(X_test)\n",
    "    observed_pred = likelihood(g)\n",
    "observed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20 5.83 9.45\n",
      "4.22 7.71 11.20\n",
      "-1.54 2.03 5.59\n",
      "3.05 6.49 9.92\n",
      "1.19 4.72 8.24\n",
      "1.42 5.14 8.85\n",
      "0.17 3.60 7.04\n",
      "2.93 6.50 10.06\n",
      "-1.71 1.73 5.17\n",
      "1.19 4.72 8.24\n",
      "2.19 5.67 9.16\n",
      "4.37 7.85 11.33\n",
      "1.71 5.19 8.66\n",
      "2.29 5.73 9.17\n",
      "4.37 7.85 11.33\n",
      "-1.75 1.85 5.45\n",
      "-1.17 2.43 6.03\n",
      "2.24 6.23 10.22\n",
      "3.05 6.49 9.92\n",
      "0.17 3.60 7.04\n",
      "2.38 5.87 9.36\n",
      "-1.87 1.59 5.05\n",
      "2.92 6.36 9.81\n",
      "3.03 6.48 9.93\n",
      "-2.94 0.68 4.30\n",
      "2.92 6.36 9.81\n",
      "2.92 6.36 9.81\n",
      "0.41 4.39 8.37\n",
      "2.73 6.16 9.60\n",
      "0.76 4.54 8.32\n",
      "1.81 5.27 8.74\n",
      "3.60 7.03 10.46\n",
      "3.60 7.03 10.46\n",
      "3.93 7.57 11.22\n",
      "1.37 4.81 8.25\n",
      "0.76 4.54 8.32\n",
      "2.13 5.57 9.01\n",
      "0.41 3.92 7.42\n",
      "0.17 3.60 7.04\n",
      "2.35 5.78 9.20\n",
      "3.05 6.71 10.36\n",
      "-2.94 0.68 4.30\n",
      "-1.79 1.86 5.51\n",
      "1.19 4.72 8.24\n",
      "2.58 6.16 9.74\n",
      "3.03 6.48 9.93\n",
      "2.35 5.78 9.20\n",
      "2.19 5.67 9.16\n",
      "4.37 7.85 11.33\n",
      "3.03 6.48 9.93\n",
      "4.55 8.21 11.87\n",
      "1.84 5.56 9.28\n",
      "4.13 7.57 11.01\n",
      "-1.89 1.68 5.26\n",
      "-1.36 2.44 6.24\n",
      "2.34 5.79 9.23\n",
      "3.61 7.08 10.55\n",
      "1.84 5.49 9.14\n",
      "-2.40 1.17 4.75\n",
      "2.92 6.36 9.81\n",
      "-1.87 1.59 5.05\n",
      "-0.81 2.85 6.50\n",
      "-1.91 1.57 5.06\n",
      "2.45 5.90 9.35\n",
      "1.14 4.71 8.27\n",
      "3.71 7.19 10.67\n",
      "4.51 7.98 11.46\n",
      "3.05 6.49 9.92\n",
      "1.03 4.73 8.43\n",
      "1.81 5.27 8.74\n",
      "1.89 5.36 8.82\n",
      "-0.56 2.97 6.51\n",
      "2.46 6.04 9.63\n",
      "4.51 7.98 11.46\n",
      "1.01 4.49 7.97\n",
      "3.63 7.11 10.59\n",
      "0.90 4.47 8.03\n",
      "1.19 4.72 8.24\n",
      "2.00 5.50 9.00\n",
      "4.51 7.98 11.46\n",
      "1.74 5.38 9.02\n",
      "-2.34 1.09 4.53\n",
      "3.03 6.48 9.93\n",
      "-2.56 1.01 4.57\n",
      "1.03 4.73 8.43\n",
      "-1.96 1.56 5.08\n",
      "2.93 6.50 10.06\n",
      "-1.75 1.85 5.45\n",
      "1.05 4.57 8.08\n",
      "-1.44 2.04 5.53\n",
      "-2.10 1.54 5.18\n",
      "1.37 4.81 8.25\n",
      "3.47 6.89 10.31\n",
      "1.63 5.07 8.52\n",
      "-2.16 1.46 5.09\n",
      "0.41 3.92 7.42\n",
      "1.02 4.70 8.37\n",
      "-2.16 1.46 5.09\n",
      "3.60 7.03 10.46\n",
      "0.75 4.30 7.85\n",
      "1.32 4.95 8.58\n",
      "2.32 5.86 9.40\n",
      "4.75 8.59 12.43\n",
      "2.38 5.87 9.36\n",
      "1.63 5.07 8.52\n",
      "4.55 8.21 11.87\n",
      "-2.75 0.92 4.59\n",
      "-2.16 1.46 5.09\n",
      "2.49 6.14 9.79\n",
      "2.68 6.21 9.74\n",
      "2.13 5.57 9.02\n",
      "3.47 6.89 10.31\n",
      "-0.81 2.85 6.50\n",
      "3.71 7.19 10.67\n",
      "0.17 3.60 7.04\n",
      "2.68 6.21 9.74\n",
      "3.03 6.48 9.93\n",
      "-0.81 2.85 6.50\n",
      "1.91 5.81 9.71\n",
      "-0.98 2.53 6.04\n",
      "1.02 4.70 8.37\n",
      "-2.40 1.17 4.75\n",
      "-2.40 1.17 4.75\n",
      "2.80 6.38 9.96\n",
      "-1.71 1.73 5.17\n",
      "2.71 6.26 9.81\n",
      "2.13 5.57 9.01\n",
      "-0.18 3.38 6.95\n",
      "3.60 7.03 10.46\n",
      "3.63 7.11 10.59\n",
      "2.13 5.57 9.02\n",
      "2.32 5.94 9.57\n",
      "-1.87 1.59 5.05\n",
      "2.34 5.79 9.23\n",
      "2.31 5.82 9.33\n",
      "0.17 3.60 7.04\n",
      "3.03 6.48 9.93\n",
      "2.92 6.36 9.81\n",
      "2.13 5.57 9.02\n",
      "3.57 7.14 10.72\n",
      "2.48 6.03 9.58\n",
      "2.13 5.57 9.01\n",
      "3.60 7.03 10.46\n",
      "3.05 6.49 9.92\n",
      "1.81 5.49 9.18\n",
      "1.02 4.70 8.37\n",
      "-2.27 1.38 5.04\n",
      "4.13 7.57 11.01\n",
      "0.76 4.54 8.32\n",
      "1.63 5.07 8.52\n",
      "-2.75 0.92 4.59\n",
      "-1.71 1.73 5.17\n",
      "3.93 7.57 11.22\n",
      "-1.87 1.59 5.05\n",
      "1.62 5.13 8.64\n",
      "2.34 5.79 9.23\n",
      "2.13 5.57 9.01\n",
      "-0.09 3.39 6.87\n",
      "-2.11 1.55 5.21\n",
      "-1.87 1.59 5.05\n",
      "2.80 6.38 9.96\n",
      "2.33 5.83 9.34\n",
      "1.05 4.57 8.08\n",
      "0.41 3.92 7.42\n",
      "1.81 5.27 8.74\n",
      "-1.71 1.73 5.17\n",
      "1.19 4.66 8.13\n",
      "3.47 6.89 10.31\n",
      "-2.34 1.09 4.53\n",
      "3.63 7.11 10.59\n",
      "0.29 3.94 7.59\n",
      "1.07 4.57 8.08\n",
      "-2.34 1.09 4.53\n",
      "1.82 5.26 8.71\n",
      "-1.38 2.21 5.81\n",
      "0.32 4.26 8.20\n",
      "-1.48 2.09 5.66\n",
      "0.78 4.36 7.94\n",
      "2.45 5.90 9.35\n",
      "6.49 10.08 13.67\n",
      "1.92 5.52 9.13\n",
      "2.13 5.57 9.01\n",
      "-1.87 1.59 5.05\n",
      "2.68 6.21 9.74\n",
      "1.82 5.26 8.71\n",
      "2.48 6.03 9.58\n",
      "1.82 5.26 8.71\n",
      "2.34 5.79 9.23\n",
      "2.13 5.57 9.01\n",
      "-2.20 1.46 5.13\n",
      "3.63 7.11 10.59\n",
      "3.60 7.03 10.46\n",
      "-1.91 1.57 5.06\n",
      "2.29 5.73 9.17\n",
      "1.37 4.81 8.25\n",
      "-1.38 2.21 5.81\n",
      "1.19 4.72 8.24\n",
      "-2.16 1.46 5.09\n",
      "2.48 6.03 9.58\n",
      "1.01 4.49 7.97\n",
      "-1.87 1.59 5.05\n",
      "-2.16 1.46 5.09\n",
      "2.73 6.16 9.60\n",
      "3.23 6.74 10.26\n",
      "2.45 5.90 9.35\n",
      "2.19 5.67 9.16\n",
      "6.49 10.08 13.67\n",
      "0.79 4.49 8.19\n",
      "0.79 4.49 8.19\n",
      "-1.93 1.69 5.32\n",
      "2.40 6.09 9.78\n",
      "6.49 10.08 13.67\n",
      "-1.44 2.04 5.53\n",
      "1.82 5.26 8.71\n",
      "1.62 5.13 8.64\n",
      "4.22 7.71 11.20\n",
      "2.34 5.79 9.23\n",
      "2.29 5.73 9.17\n",
      "1.63 5.07 8.52\n",
      "1.53 5.18 8.82\n",
      "1.76 5.36 8.96\n",
      "2.80 6.38 9.96\n",
      "-2.40 1.17 4.75\n",
      "2.04 5.55 9.06\n",
      "3.60 7.03 10.46\n",
      "-3.08 0.58 4.23\n",
      "0.72 4.67 8.61\n",
      "2.38 5.87 9.36\n",
      "2.34 5.79 9.23\n",
      "2.19 5.67 9.16\n",
      "2.34 5.79 9.23\n",
      "-0.18 3.38 6.95\n",
      "-2.23 1.28 4.80\n",
      "3.03 6.48 9.93\n",
      "2.93 6.50 10.06\n",
      "1.63 5.07 8.52\n",
      "4.13 7.57 11.01\n",
      "1.49 5.09 8.70\n",
      "2.46 6.04 9.63\n",
      "2.13 5.57 9.02\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "    mean = observed_pred.mean.numpy()\n",
    "\n",
    "for l,u,m in zip(lower,upper,mean):\n",
    "    print(f\"{l:0.02f} {m:0.02f} {u:0.02f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
